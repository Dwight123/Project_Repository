{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Name: Dwight Devens\n",
    "#### Student ID: A15711217"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 190 Final Project\n",
    "\n",
    "### Suggested Project 1\n",
    "\n",
    "### Composing Melody Using RNN with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/dwight/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.io import wavfile\n",
    "from numpy.linalg import svd\n",
    "from scipy.stats.mstats import gmean\n",
    "from matplotlib import rcParams\n",
    "import scipy\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization as BatchNorm\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from keras_sequential_ascii import keras2ascii\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "numberOfNotesForPrediction = 8\n",
    "numberOfEpochsToTrainFor = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parses each midi file and extracts the individual notes\n",
    "def parseNotes():\n",
    "\n",
    "    notesAlreadyParsed = []\n",
    "    # For each midi file\n",
    "    for file in glob.glob(\"MyData/*.mid\"):\n",
    "        # Reset the note counter\n",
    "        noteCounter = None\n",
    "        # Parse the file\n",
    "        midi = converter.parse(file)\n",
    "        # If the file has instruments in it\n",
    "        try:\n",
    "            checkInstrument = instrument.partitionByInstrument(midi)\n",
    "            noteCounter = checkInstrument.parts[0].recurse()\n",
    "        # And if that fails\n",
    "        except:\n",
    "            noteCounter = midi.flat.notes\n",
    "        # Then for each note\n",
    "        for thisNote in noteCounter:\n",
    "            # If it is a chord\n",
    "            if isinstance(thisNote, chord.Chord):\n",
    "                notesAlreadyParsed.append('.'.join(str(n) for n in element.normalOrder))\n",
    "            # Else it is a note\n",
    "            elif isinstance(thisNote, note.Note):\n",
    "                notesAlreadyParsed.append(str(thisNote.pitch))\n",
    "    \n",
    "    # Done with files, open output and write it\n",
    "    with open('MyData/notes', 'wb') as filepath:\n",
    "        pickle.dump(notesAlreadyParsed, filepath)\n",
    "    return notesAlreadyParsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructMelodies(notesAlreadyParsed, n_vocab, numberOfNotesforPrediction):\n",
    "    \n",
    "    # Sort the pitches\n",
    "    sortedPitches = sorted(set(item for item in notesAlreadyParsed))\n",
    "    # Map them to ints\n",
    "    noteIntegerDict = dict((note, number) for number, note in enumerate(sortedPitches))\n",
    "    networkInput = []\n",
    "    networkOutput = []\n",
    "    \n",
    "    # Create the input/output to the network\n",
    "    for i in range(0, len(notesAlreadyParsed) - numberOfNotesforPrediction, 1):\n",
    "        # Input\n",
    "        inputSequence = notesAlreadyParsed[i:i + numberOfNotesforPrediction]\n",
    "        networkInput.append([noteIntegerDict[char] for char in inputSequence])\n",
    "        # Output\n",
    "        outputSequence = notesAlreadyParsed[i + numberOfNotesforPrediction]\n",
    "        networkOutput.append(noteIntegerDict[outputSequence])\n",
    "\n",
    "    lengthOfNetwork = len(networkInput)\n",
    "\n",
    "    # Had to change shape for LSTM layers or it wasn't working\n",
    "    networkInput = np.reshape(networkInput, (lengthOfNetwork, numberOfNotesforPrediction, 1))\n",
    "    networkInput = networkInput / float(n_vocab)\n",
    "    # Other utils wasn't working leave this with current versions\n",
    "    networkOutput = np_utils.to_categorical(networkOutput)\n",
    "    return (networkInput, networkOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructNetwork(networkInput, n_vocab):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, input_shape=(networkInput.shape[1], networkInput.shape[2]),\n",
    "                   recurrent_dropout=0.3, return_sequences=True))\n",
    "    model.add(LSTM(512, return_sequences=True, recurrent_dropout=0.3,))\n",
    "    \n",
    "    # 2. A batch normalization layer. \n",
    "    # Keras documentation - \"Layer that normalizes its inputs. Batch normalization \n",
    "    # applies a transformation that maintains the mean output close to 0 and the output \n",
    "    # standard deviation close to 1.\"\"\n",
    "    # Example: Args look optional, just instantiate for now\n",
    "    model.add(BatchNorm())\n",
    "    \n",
    "    # 3. A layer which drops 3/10 of the units. \n",
    "    # Keras documentation - \"The Dropout layer randomly sets input units to 0 with a \n",
    "    # frequency of rate at each step during training time, \n",
    "    # which helps prevent overfitting. \n",
    "    # Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all \n",
    "    # inputs is unchanged.\"\"\n",
    "    # Example: tf.keras.layers.Dropout(rate, noise_shape=None, seed=None, **kwargs)\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # 4. A fully connected layer with 256 units of output. \n",
    "    # Keras documentation - \"Just your regular densely-connected NN layer.\"\n",
    "    # Example: model.add(tf.keras.layers.Dense(32))\n",
    "    model.add(Dense(256))\n",
    "    \n",
    "    # 5. A ReLU activation layer. \n",
    "    # Keras documentatino - \"Applies an activation function to an output.\n",
    "    # Arguments - activation: Activation function, such as tf.nn.relu, \n",
    "    # or string name of built-in activation function, such as \"relu\".\"\n",
    "    # Example: layer = tf.keras.layers.Activation('relu')\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # 6. A batch normalization layer. \n",
    "    # Keras documentation - \"Layer that normalizes its inputs. Batch normalization \n",
    "    # applies a transformation that maintains the mean output close to 0 and the output \n",
    "    # standard deviation close to 1.\"\"\n",
    "    # Example: Args look optional, just instantiate for now\n",
    "    model.add(BatchNorm())\n",
    "    \n",
    "    # 7. A layer which drops 3/10 of the units. \n",
    "    # Keras documentation - \"The Dropout layer randomly sets input units to 0 with a \n",
    "    # frequency of rate at each step during training time, \n",
    "    # which helps prevent overfitting. \n",
    "    # Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all \n",
    "    # inputs is unchanged.\"\"\n",
    "    # Example: tf.keras.layers.Dropout(rate, noise_shape=None, seed=None, **kwargs)\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # 8. A fully connected layer with number of units of output equal to the vocabulary \n",
    "    # space of the input. \n",
    "    # Keras documentation - \"Just your regular densely-connected NN layer.\"\n",
    "    # Example: model.add(tf.keras.layers.Dense(32))\n",
    "    # but this time need vocab space of input, which = n_vocab  \n",
    "    model.add(Dense(n_vocab))\n",
    "    \n",
    "    # 9. A softmax activation layer which uses a temperature of .6 \n",
    "    # (Note, you may need to define this as two separate layers in Keras, \n",
    "    # using the definition of temperature for softmax)\n",
    "    \n",
    "    # Keras documentation - \"Softmax converts a vector of values to a probability \n",
    "    # distribution. The elements of the output vector are in range (0, 1) and sum to 1.\n",
    "    # Each vector is handled independently. The axis argument sets which axis of the \n",
    "    # input the function is applied along. Softmax is often used as the activation \n",
    "    # for the last layer of a classification network because the result could be \n",
    "    # interpreted as a probability distribution. The softmax of each vector x is \n",
    "    # computed as exp(x) / tf.reduce_sum(exp(x)). The input values in are the log-odds \n",
    "    # of the resulting probability.\n",
    "    # Arguments - x : Input tensor. axis: Integer, axis along which the softmax \n",
    "    # normalization is applied.\"\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # After creating your network, compile the model with categorical cross entropy loss \n",
    "    # and an optimizer of your choice. \n",
    "    # Example: model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "    # Load the weights to each node\n",
    "    model.load_weights(filePathToWeights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTheNetwork(numberOfNotesforPrediction, numberOfEpochsToTrainFor):\n",
    "    # Parse the notes and store the amount\n",
    "    notesAlreadyParsed = parseNotes()\n",
    "    n_vocab = len(set(notesAlreadyParsed)) \n",
    "    # Construct the melodies\n",
    "    networkInput, networkOutput = constructMelodies(notesAlreadyParsed, n_vocab, numberOfNotesforPrediction)\n",
    "    # Build the network\n",
    "    networkModel = buildTheNetwork(networkInput, n_vocab)\n",
    "    # Train the network\n",
    "    weightsFile = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "    checkpoint = theModelCheckpoint(weightsFile, monitor='loss', verbose=0,\n",
    "                                save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    networkModel.fit(networkInput, networkOutput, \n",
    "                 epochs=numberOfEpochsToTrainFor, batch_size=128, callbacks=callbacks_list)\n",
    "    \n",
    "    print(\"done training\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTheNetwork(numberOfNotesForPrediction, numberOfEpochsToTrainFor)\n",
    "print(\"end\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
